---
title: "Linear Models"
author: "Math 246"
date: "January 29, 2018"
output: ioslides_presentation
---

## Note

We'll use the cdc dataset a little longer. As we model this, be sure to try it with the dataset (porche) used in the book

# Linear Models (1.1)

## Notation

$$Data = Model + Error$$
$$Y = f(X) + \epsilon$$
$$Y = \mu_Y + \epsilon$$
$$\mu_Y = f(X) = \beta_0+\beta_1 X$$
$$Y = \beta_0 + \beta_1 X + \epsilon$$

## Making a plot

We'll use the `ggplot2` package (can also use "base" graphics), use `require(ggplot2)` and load `cdc` dataset.

```{r include=FALSE}
require(ggplot2)
require(dplyr)
cdc <- read.csv("~/Dropbox/teaching/246/data/cdc.csv")
```

## Making a quick plot

```{r}
qplot(height, weight, data=cdc)
```

## Adding a best-fit line

```{r}
qplot(height, weight, data=cdc)+geom_smooth(method="lm", se=FALSE)
```

## Method of finding best fit line

* SSE - sum of squares of the error
* least sqaures line

## Getting the equation for that line

```{r}
lm(weight~height, data=cdc)
```

## More Info {.smaller}
```{r}
summary(lm(weight~height, data=cdc))
```
What's the equation of the best-fitting line?
What do the values mean?

## dplyr {.smaller}

Of course - there's a dplyr way to do this with pipes (if you like them)

```{r}
cdc %>% lm(weight~height, data=.) %>% summary()
```

## What Residuals Mean

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$
$$weight_i = \beta_0 + \beta_1 * height_i + \epsilon$$
$$\widehat{weight} = \beta_0 + \beta_1* height$$

## Residuals

```{r}
mod1 <- lm(weight~height, data=cdc)
mod1$residuals
```

## Summarize residuals {.smaller}

```{r}
summary(mod1$residuals)
```

## Plot residuals {.smaller}

```{r}
qplot(mod1$residuals)
```

# Conditions for Linear Models (1.2)

## Conditions

"We hope that the residuals are small and contain no pattern that could be exploited to better explain the response variable."

## Linearity

"The overall relationship between the variables has a linear pattern. The average values of the response Y for each value of X fall on a common straight line."

(Plot them)

## Zero Mean (of residuals)

"The error distribution is centered at zero. This means that the points are scattered at random above and below the line. (Note: By using least squares regression, we force the residual mean to be zero. Other techniques would not necessarily satisfy this condition.)"

## Constant Variance (homoscedasticity)

"The variability in the errors is the same for all values of the predictor variable. This means that the spread of points around the line remains fairly constant."

## Testing constant variance

```{r}
mod2 <- lm(weight~height, data=cdc)
qplot(height, mod2$residuals, data=cdc)
```

## Another example
```{r}
mod3 <- lm(height~age, data=cdc)
qplot(age,mod3$residuals, data=cdc)
```

## Independence
"The errors are assumed to be independent from one another. Thus, one point falling above or below the line has no influence on the location of another point."

## Inference Assumptions

"When we are interested in using the model to make formal inferences (conducting hypothesis tests or providing confidence intervals), additional assumptions are needed."

## Random
"The data are obtained using a random process. Most commonly, this arises either from random sampling from a population of interest or from the use of randomization in a statistical experiment."

## Normality
"In order to use standard distributions for confidence intervals and hypothesis tests, we often need to assume that the random errors follow a normal distribution."

## Testing normality

```{r message=FALSE}
qplot(mod3$residuals)
```

---

```{r message=FALSE}
qplot(mod2$residuals)
```

## Standard Errors with a toy dataset

```{r}
mtcars
```

## Regression standard error

Standard deviation of the error term

$$\widehat{\sigma_\epsilon} = \sqrt{\frac{SSE}{n-2}}$$

---

```{r}
mtcars %>% lm(mpg~wt, data=.) %>% summary()
```

```{r}
qplot(wt, mpg, data=mtcars) +
    geom_smooth(method="lm")
```

## Where we are

* Github 
* Pick your own dataset: lab 0
* RMarkdown

## Assumptions
* Assumptions vs conditions

Not standard, but I consider:
* Randomly collected data - assumption
* Homoscedasticity or normality - condition

# Assessing Conditions (1.3)

## Residuals vs fits

* We already looked at residuals vs predictor
* Now we look at residual vs predicted
* The book has some nice graphs of what these can look like, we'll generate some

## Example {.smaller}

Reminder:

```{r}
summary(mod2)
```

---

```{r}
qplot(mod2$fitted.values, mod2$residuals)
```

## Making up some data {.smaller}

```{r}
x <- rnorm(50,5,2)
noise <- rnorm(50,0,3)
y <- 2*x^2+4+x+3+noise
qplot(x,y)
```

## Running a lm {.smaller}

```{r}
randmod <- lm(y~x)
summary(randmod)
```

## Assessing the fit

```{r}
qplot(x,randmod$residuals)
```

---

```{r}
qplot(randmod$fitted.values, randmod$residuals)
```

---

```{r message=FALSE, warning=FALSE}
qplot(randmod$residuals)
```

## qqplots (normal plot)

```{r}
qqnorm(mod2$residuals)
```

---

```{r}
qqnorm(mod3$residuals)
```

---

```{r}
qqnorm(randmod$residuals)
```

## Perfect qqnorm plot

```{r}
normaldata <- rnorm(500,34,17)
qqnorm(normaldata)
```

## Model assessment

* Linearity
* Zero mean
* Constant variance
* Independent and random
* Normality

## Model use: Making predictions {.smaller}

```{r}
summary(mod3)
```

---

Suppose age is 27:
```{r}
new <- data.frame(age=c(27, 28, 29))
predict.lm(mod3, new)
```

## Note

Recommended datacamp (not required) course: Correlation and Regression

# Transformations (1.4)

## What to do when the lm assumptions/conditions aren't satisfied?

* In particular, normality of residuals and/or constant variance of residuals
* Can also be useful if we can "see" that the relationship is not linear

## The usual tools

* Transform one or the other variable using:
  * Square
  * Square root
  * Inverse
  * log (log in R is ln)
  
## Example Dataset

```{r}
wordrecall <- read.delim("~/Dropbox/teaching/246/data/wordrecall.txt")
wordrecall <- tbl_df(wordrecall)
wordrecall
```

## Model {.smaller}
```{r}
wordmodel <- lm(prop ~ time, data=wordrecall)
summary(wordmodel)
```


## Graphs

```{r}
wordrecall %>% qplot(time, prop, data=.)+geom_smooth(method="lm", se=F)
```

---

```{r}
qplot(wordmodel$fitted.values, wordmodel$residuals)
```

---

```{r}
qqnorm(wordmodel$residuals)
```


## How do we fix this?

* Transform the variables
* Use mutate() to construct new variables which might work better, e.g.: log of prop, log of time, prop^-1.25, etc.
* What works well?

```{r}
wordrecall <- wordrecall %>% mutate(logprop = log(prop))
wordrecall
```

## Using our best model, let's make some prediction

Pick a time. What is the expected proportion of words recalled at that time?

## Note

Taking the square root of y is not the same as a quadratic regression. What's the difference?

# 1.5 Outliers and where to find them

## Terms

* Standardized residuals: make residuals into z-scores
* Studentized (deleted-t) residuals: remove outliers to find std error, then determine residual

## Reminder {.smaller}

```{r}
summary(wordmodel)
```

---

```{r}
stdres <- rstandard(wordmodel)
stdres
```

---

```{r}
qplot(wordmodel$fitted.values, stdres)
```

---

```{r}
studres <- rstudent(wordmodel)
studres
```

---

```{r}
qplot(wordmodel$fitted.values, studres)
```

## Another example

* The PalmBeach dataset is an excellent example. I'll also show that we don't have to download the data to our computer first:

```{r}
library(readr)
PalmBeach <- read_csv("http://stat2.org/datasets/PalmBeach.csv")
```

(Note there are also rda files on the stat2 website!)

How can we recreate the analysis with and without the outlier in R?

## Terms

* influence: how well the point aligns to the trend
* leverage: how far the point ($x$) is from the mean of the predictors ($\bar{x}$)
