---
title: "Multiple Regression"
author: "Math 246"
date: "February 21, 2018"
output: 
  ioslides_presentation: 
    widescreen: yes
---

## Back to the CDC dataset!! {.smaller}

```{r include=FALSE}
library(tidyverse)
library(readr)
cdc <- read_csv("../data/cdc.csv")
```

Predict height based on smoke100, how do we interpret the slope?

```{r}
cdc %>% lm(height~smoke100, data=.) %>% summary()
```

## Another model: predict height by exerany {.smaller}

```{r}
cdc %>% lm(height~exerany, data=.) %>% summary()
```

## Form of the model

Note: this is a model for the population (no hats!):

$$\mu_Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + \ldots + \beta_k X_k$$

$$Y = \beta_0 + \beta_1 X_1 +\beta_2 X_2 + \ldots + \beta_k X_k + \epsilon$$

What's the difference?

## How to run it {.smaller}

What is the equation for the line of best fit?

```{r}
cdc %>% lm(height~exerany+smoke100, data=.) %>% summary()
```

## The new anova table

Check df - we now have 2 degrees taken up by the predictors, so we've got $n-2-1$ for the df of the residuals. (How many values are we predicting from our model?)

```{r}
cdc %>% lm(height~exerany+smoke100, data=.) %>% anova()
```

## The new standard error of the multiple regression model:

$$\hat{\sigma_\epsilon} = \sqrt{\frac{SSE}{df}} = \sqrt{\frac{SSE}{n-k-1}}$$

where we have $k$ predictors.

## More multivarite models  {.smaller}

How do we interpret these coefficients? What is the equation of the regression line?

```{r}
cdc %>% lm(height~age+exerany, data=.) %>% summary()
```

## One more model {.smaller}

```{r}
cdc %>% lm(height~age+weight, data=.) %>% summary()
```

## Age and weight {.smaller}

Is this the same as weight=0?

```{r}
cdc %>% lm(height~age, data=.) %>% summary()
```


## Correlation in the predictors

```{r}
cor.test(cdc$age, cdc$weight)
```

---

```{r}
cdc %>% qplot(age,weight, data=.) + geom_smooth(method="lm")
```

## Plotting

These can be plotted in 3-d graphs, but I find they are not very helpful

## Assumptions

* Independence
* Constant variance for *any combination* of the predictors

## An aside

Interaction terms aren't treated in the text here, but I think this is a good place to talk about them

## Reminder {.smaller}

```{r}
cdc %>% lm(height~exerany+smoke100, data=.) %>% summary()
```

## New Model {.smaller}

```{r}
cdc %>% lm(height~exerany*smoke100, data=.) %>% summary()
```

## Reminder {.smaller}

```{r}
cdc %>% lm(height~age+exerany, data=.) %>% summary()
```

## New Model {.smaller}

```{r}
cdc %>% lm(height~age*exerany, data=.) %>% summary()
```

## Reminder {.smaller}

```{r}
cdc %>% lm(height~age+weight, data=.) %>% summary()
```

## New Model {.smaller}

```{r}
cdc %>% lm(height~age*weight, data=.) %>% summary()
```

## Quick question

Why did I distinguish between predictors and explanatory variables earlier in the course?

## Try some

Use the imdb movies dataset on the website to create a multiple regression model - you pick the predictors and response variables

# Assessing Model Fit (3.2)

## Hypothesis Tests

How do we set up hypothesis tests for multiple regression?

Note: What do we do with a predictor that isn't statistically significant? How do we find "the best model"?

## t-test

The t-test has $n-k-1$ degrees of freedom when there are $k$ predictors. Why does this make sense?

## Confidence intervals

$\hat{\beta_i} \pm t^* SE_{\hat{\beta_i}}$

## ANOVA tables

* One table for the model
* $SSModel = \sum (\hat{y}-\bar{y})^2$
* $SSE = \sum (y-\hat{y})^2$
* $SSTotal = \sum (y - \bar{y})^2$

## ANOVA table

Model is all the lines up to the residuals

```{r}
cdc %>% lm(height~age*exerany, data=.) %>% anova()
```

This looks different than the book...

## Getting the book results...

```{r}
NFL <- read_csv("http://stat2.org/datasets/NFL2007Standings.csv")
```

---

```{r}
NFL %>% lm(WinPct~PointsFor+PointsAgainst, data=.) %>% anova()
```

## {.smaller}

```{r}
NFL %>% lm(WinPct~PointsFor+PointsAgainst, data=.) %>% summary()
```

## Hang on to your seats

```{r}
forFirst <- NFL %>% lm(WinPct~PointsFor+PointsAgainst, data=.)
againstFirst <- NFL %>% lm(WinPct~PointsAgainst+PointsFor, data=.)
```

What's the difference?

## Sumamries {.smaller}

```{r}
summary(forFirst)
```

## {.smaller}

```{r}
summary(againstFirst)
```

## ANOVA tables

```{r}
anova(forFirst)
```

---

```{r}
anova(againstFirst)
```

## Meet the new $R^2$, same as the old $R^2$

$R^2$ is the coefficient of determiniation (this *was* the square of the correlation coefficient)

$$R^2 = \frac{variability \ explained \ by \ model}{total \ variability \ in \ y} = \frac{SSModel}{SSTotal} = 1 - \frac{SSE}{SSTotal}$$

What is our correlation coefficient now?

## {.smaller}

```{r}
summary(forFirst)
```

## Where does it come from?

```{r}
r <- cor(forFirst$fitted.values, NFL$WinPct)
r
r^2
```

## Adjusted $R^2$

$$R^2_{adj} = 1-\frac{SSE/(n-k-1)}{SSTotal/(n-1)} = 1-\frac{\hat{\sigma_\epsilon}}{S^2_Y}$$

# Comparing Regression Lines (3.3)

## New stuff?

Not really - we've already been doing this

An *indicator variable* uses two values to denote group membership (usually 0 and 1)

How do we compare the slopes of two different groups when we have an indicator variable? How do we tell whether they are statistically significantly different?