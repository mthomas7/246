---
title: "Inference for Regression"
author: "Math 246"
date: "February 13, 2018"
output: ioslides_presentation
---

## Some theoretical concerns

The coefficients in a linear regression are given by:
$$\hat{\beta}_0 = \bar{y}-\hat{\beta_1} \bar{x}$$

$$\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2}$$

Show that:

* $(\bar{x}, \bar{y})$ is on the regression line
* $\sum y_i-\hat{y}_i=0$

# Inference for Regression Slope (2.1)

## A slight detour - t-tests

```{r warning=TRUE, include=FALSE}
require(dplyr)
require(ggplot2)
cdc <- read.csv("~/Dropbox/teaching/246/data/cdc.csv")
```

Loaded dplyr and cdc dataset:

```{r}
cdc %>% t.test(height~exerany, data=.)
```

## Regression version {.smaller}

```{r}
cdc %>% lm(height~exerany, data=.) %>% summary()
```

## What's going on?

Regression: $\widehat{height} = 66.38832+1.06555*\textrm{exerany}$

Whether the slope is "meaningful" is the same question as whether there's a difference between groups. (That's a t-test)

Recall a 2-sample t-test: $$t = \frac{\hat{x_1}-\hat{x_2}}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}} = \frac{\textrm{diff of means}}{\textrm{std error}}$$

## The Continuous Case {.smaller}

```{r}
cdc %>% lm(height~age, data=.) %>% summary()
```

## The values

$t = \frac{\hat{\beta_1}}{SE_{\hat{\beta_1}}}$

IMPORTANT: STANDARD ERROR OF SLOPE $SE_{\hat{\beta_1}}$ $\neq$ STANDARD ERROR OF REGRESSION $\hat{\sigma}_\epsilon$

$$\widehat{\sigma_\epsilon} = \sqrt{\frac{SSE}{n-2}}, \ SE_{\hat{\beta_1}} = \sqrt{\frac{\sum (y_i - \hat{y}_i)^2 /(n-2)}{\sum (x_i - \bar{x})^2}}$$

## The formal test

Using conditions/assumptions for simple linear model:

$H_0: \beta_1 = 0$

$H_A: \beta_1 \neq 0$

With test statistic $t = \frac{\hat{\beta_1}}{SE_\hat{\beta_1}}$, $n-2$ degrees of freedom

## Asides

* You can do a 1-tailed test, but this isn't typically done in regression
* You can remove the intercept ($y = \beta_1*x + \epsilon$) but you should have a good reason to do so
* You can do a similar test for intercept, though it is usually less interesting. The slope is usually where the interesting stuff is happening

## Question

The standard deviation of the error or regression standard error, $\widehat{\sigma_\epsilon}$, isn't *quite* the 
"average of the errors." Why?

## Confidence Intervals

```{r}
cdc %>% t.test(height~exerany, data=.)
```

---

```{r}
cdc %>% lm(height~exerany, data=.) %>% summary()
```

## Formulas

Confidence intervals:
$ \hat{\beta_1} \pm t^* * SE_{\hat{\beta_1}}$

To get $t^*$:
```{r}
qt(.975, 19998)
```


# ANOVA (2.2)

## How it works

Analysis of variance is all about splitting up variation in Y between the model and the residuals. We'll walk through the equations in the section.

$y - \bar{y} = (\hat{y}-\bar{y}) + (y - \hat{y})$ (model and residual)

ANOVA sum of squares identity:

$\sum (y - \bar{y})^2 = \sum (\hat{y} - \bar{y})^2 + \sum(y - \hat{y})^2$

(this is not obvious)

SSTotal = SSModel + SSE

## What's going on?

```{r}
cdc %>% qplot(age, height, data=.) + geom_smooth(method="lm", se=F)
```


## Example {.smaller}

```{r}
mod1 <- lm(height~age, data=cdc)
summary(mod1)
```

---

```{r}
mod1 <- lm(height~age, data=cdc)
anova(mod1)
```
 
Can also do:
 
`cdc %>% lm(height~age, data=.) %>% anova()`
 
if you want to use pipes
 
## Interpreting
 
How do we interpret this table?
 
## Drawing distributions
 
This isn't *necessary*, but it's cool.
 
```{r}
curve(dnorm(x),-5,5)
```
 
 ---
 
```{r}
curve(dt(x,30),-5,5,col="red")
curve(dt(x,1),-5,5, add=TRUE, col="green")
```
 
 ---
 
```{r}
curve(df(x,1,19998),0,5)
```
 
## The degrees of freedom

* The entire model has $n-2$ degrees of freedom.
* SSTotal has $n-1$ degrees of freedom (this has estimated a mean)
* SSModel has $1$ degree of freedom (this is the slope).
* Note: $n-1= 1+(n-2)$, so the df of SSTotal is the sum of the df of SSModel and SSE (SS residuals)

## Try this on your data set for lab 0

# Correlation (2.3)

## The values of interest

Coefficient of determination: $r^2$

Correlation coefficient: $r$

## Definitions

$$r^2 = \frac{\sum (\hat{y}-\bar{y})^2}{\sum (y - \bar{y})^2} = \frac{SSModel}{SSTotal}$$

What values can $r^2$ be?

$$\hat{\beta_1} = r \frac{s_Y}{s_X}$$

What values can $r$ be?

## t-test for correlation

Let $\rho$ be the correlation. What is the null hypothesis? What is the alternative hypothesis?

Assuming conditions for the simple linear model hold, 
$$t = \frac{r \sqrt{n-2}}{\sqrt{1-r^2}}, \ df=n-2$$

## Making sense of this:

Let's use the goldenrod data
```{r include=FALSE}
require(readr)
require(ggplot2)
require(dplyr)
Goldenrod <- read_csv("http://stat2.org/datasets/Goldenrod.csv")
```

```{r}
Goldenrod
```

## Linear model for '04 {.smaller}

```{r}
Goldenrod %>% lm(Wall04~Gdiam04, data=.) %>% summary()
```

## Correlation

```{r}
cor.test(Goldenrod$Wall04,Goldenrod$Gdiam04)
```

## What happens if you switch the two variables?

(Be careful of syntax)

## Github in groups

# Intervals for Prediction (2.4)

## Confidence intervals for regression

```{r}
Goldenrod %>% qplot(Gdiam04, Wall04, data=.)
```

```{r}
Goldenrod %>% qplot(Gdiam04, Wall04, data=.)+geom_smooth(method="lm")
```

## Why the band? {.smaller}

A confidence interval for the mean response $\mu_Y$ when X takes on the value $x^*$ is:
$$\hat{y} \pm t^* \cdot SE_{\hat{\mu}}$$
where
$$SE_{\hat{\mu}} = \hat{\sigma_\epsilon} \sqrt{\frac{1}{n}+\frac{(x^*-\bar{x})^2}{\sum (x - \bar{x})^2}}$$
Which then has prediction interval for a single observation of Y when X takes on the value $x^*$:
$$\hat{y} \pm t^* SE_{\hat{y}}$$
Where
$$SE_\hat{y} = \hat{\sigma_\epsilon} \sqrt{1+\frac{1}{n}+\frac{(x^*-\bar{x})^2}{\sum (x - \bar{x})^2}}$$

